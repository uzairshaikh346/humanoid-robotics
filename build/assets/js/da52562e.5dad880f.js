"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[425],{963:(e,n,r)=>{r.d(n,{A:()=>s});r(6540);var t=r(4164);const o={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var i=r(4848);function s({type:e,children:n}){const r=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),s=(0,t.A)("callout",o.callout,o[e]);return(0,i.jsxs)("div",{className:s,children:[(0,i.jsx)("div",{className:o.icon,children:r}),(0,i.jsx)("div",{className:o.content,children:n})]})}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},8807:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>f,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vla/chapter-17-voice-to-action","title":"Chapter 17 - Voice to Action","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-17-voice-to-action.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-17-voice-to-action","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-17-voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 17 - Voice to Action"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 16 - LLMs and Robotics","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-16-llms-and-robotics"},"next":{"title":"Chapter 18 - Cognitive Planning","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-18-cognitive-planning"}}');var o=r(4848),i=r(8453),s=r(963);const a={sidebar_position:2,title:"Chapter 17 - Voice to Action"},c="Chapter 17: Voice to Action Systems",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-17-voice-to-action-systems",children:"Chapter 17: Voice to Action Systems"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Build voice-to-action systems for robotics applications"}),"\n",(0,o.jsx)(n.li,{children:"Integrate speech recognition with robot control"}),"\n",(0,o.jsx)(n.li,{children:"Implement context-aware voice command interpretation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,o.jsx)(n.p,{children:"Voice to action systems enable robots to understand and execute spoken commands, providing a natural interface for human-robot interaction."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan\r\nimport time\r\n\r\nclass VoiceToActionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_to_action\')\r\n        \r\n        # Initialize speech recognizer\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        # Set up ROS publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.speech_pub = self.create_publisher(String, \'/tts_input\', 10)\r\n        \r\n        # Subscribe to sensor data for context awareness\r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan, \r\n            \'/scan\', \r\n            self.lidar_callback, \r\n            10\r\n        )\r\n        \r\n        # Robot state\r\n        self.lidar_data = None\r\n        self.last_command_time = time.time()\r\n        \r\n        # Start voice recognition\r\n        self.get_logger().info(\'Starting voice recognition...\')\r\n        self.listen_for_commands()\r\n\r\n    def lidar_callback(self, msg: LaserScan):\r\n        """Update lidar data for context awareness"""\r\n        self.lidar_data = msg.ranges\r\n\r\n    def listen_for_commands(self):\r\n        """Continuously listen for voice commands"""\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n        \r\n        self.get_logger().info(\'Listening for commands...\')\r\n        \r\n        while rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    # Listen for audio with timeout\r\n                    audio = self.recognizer.listen(source, timeout=5.0, phrase_time_limit=5.0)\r\n                \r\n                # Recognize speech using Google Web Speech API\r\n                command_text = self.recognizer.recognize_google(audio).lower()\r\n                self.get_logger().info(f\'Heard command: {command_text}\')\r\n                \r\n                # Process the command\r\n                self.process_voice_command(command_text)\r\n                \r\n            except sr.WaitTimeoutError:\r\n                # No speech detected within timeout, continue listening\r\n                pass\r\n            except sr.UnknownValueError:\r\n                self.get_logger().info(\'Could not understand audio\')\r\n            except sr.RequestError as e:\r\n                self.get_logger().error(f\'Error with speech recognition service: {e}\')\r\n            except Exception as e:\r\n                self.get_logger().error(f\'Unexpected error: {e}\')\r\n\r\n    def process_voice_command(self, command: str):\r\n        """Process recognized voice command"""\r\n        # Context-aware processing\r\n        if \'stop\' in command:\r\n            self.stop_robot()\r\n        elif \'forward\' in command or \'go\' in command:\r\n            if self.is_path_clear():\r\n                self.move_forward()\r\n            else:\r\n                self.say("Path is blocked, cannot move forward")\r\n        elif \'backward\' in command:\r\n            self.move_backward()\r\n        elif \'turn left\' in command or \'left\' in command:\r\n            self.turn_left()\r\n        elif \'turn right\' in command or \'right\' in command:\r\n            self.turn_right()\r\n        elif \'spin\' in command:\r\n            self.spin()\r\n        elif \'help\' in command:\r\n            self.provide_help()\r\n        else:\r\n            self.say(f"I didn\'t understand the command: {command}")\r\n        \r\n        self.last_command_time = time.time()\r\n\r\n    def is_path_clear(self) -> bool:\r\n        """Check if path ahead is clear using lidar data"""\r\n        if self.lidar_data is None:\r\n            return True  # If no data available, assume path is clear\r\n        \r\n        # Check the front 30-degree sector\r\n        front_ranges = self.lidar_data[len(self.lidar_data)//2 - 15 : len(self.lidar_data)//2 + 15]\r\n        min_distance = min([r for r in front_ranges if r != float(\'inf\')], default=float(\'inf\'))\r\n        \r\n        # Path is clear if no obstacles within 1 meter\r\n        return min_distance > 1.0\r\n\r\n    def stop_robot(self):\r\n        """Stop the robot"""\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.0\r\n        cmd.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Stopping robot")\r\n\r\n    def move_forward(self):\r\n        """Move robot forward"""\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.5  # m/s\r\n        cmd.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Moving forward")\r\n\r\n    def move_backward(self):\r\n        """Move robot backward"""\r\n        cmd = Twist()\r\n        cmd.linear.x = -0.5  # m/s\r\n        cmd.angular.z = 0.0\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Moving backward")\r\n\r\n    def turn_left(self):\r\n        """Turn robot left"""\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.0\r\n        cmd.angular.z = 0.5  # rad/s\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Turning left")\r\n\r\n    def turn_right(self):\r\n        """Turn robot right"""\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.0\r\n        cmd.angular.z = -0.5  # rad/s\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Turning right")\r\n\r\n    def spin(self):\r\n        """Spin robot in place"""\r\n        cmd = Twist()\r\n        cmd.linear.x = 0.0\r\n        cmd.angular.z = 1.0  # rad/s\r\n        self.cmd_vel_pub.publish(cmd)\r\n        self.say("Spinning")\r\n\r\n    def say(self, text: str):\r\n        """Publish text for TTS system"""\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n        self.get_logger().info(f"Robot says: {text}")\r\n\r\n    def provide_help(self):\r\n        """Provide help on available commands"""\r\n        help_text = "Available commands: move forward, turn left, turn right, spin, stop, help"\r\n        self.say(help_text)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    \r\n    # Set up speech recognition parameters\r\n    recognizer = sr.Recognizer()\r\n    recognizer.energy_threshold = 4000  # Adjust for ambient noise\r\n    \r\n    try:\r\n        voice_node = VoiceToActionNode()\r\n        rclpy.spin(voice_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\r\n    A[Human Speech] --\x3e B[Speech Recognition]\r\n    B --\x3e C[Natural Language Understanding]\r\n    C --\x3e D[Context Integration]\r\n    D --\x3e E[Action Planning]\r\n    E --\x3e F[Robot Action Execution]\r\n    G[Robot Sensors] --\x3e D\r\n    H[Environment Context] --\x3e D\r\n    I[Robot State] --\x3e D\r\n    J[Speech Output] --\x3e A\n"})}),"\n",(0,o.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,o.jsx)(s.A,{type:"info",children:(0,o.jsx)(n.p,{children:"Context-aware voice systems consider sensor data and robot state when interpreting commands, making them more robust and useful."})}),"\n",(0,o.jsx)(s.A,{type:"tip",children:(0,o.jsx)(n.p,{children:"Implement timeouts and error handling in voice recognition systems to prevent the robot from getting stuck waiting for commands."})}),"\n",(0,o.jsx)(s.A,{type:"caution",children:(0,o.jsx)(n.p,{children:"Voice recognition accuracy can vary with ambient noise, accents, and other factors. Always provide feedback to users about recognized commands."})}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Build a voice-controlled robot in simulation with safety constraints"}),"\n",(0,o.jsx)(n.li,{children:"Implement context-awareness based on sensor data for voice commands"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate voice recognition performance in different acoustic environments"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Voice-to-action systems provide intuitive human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Context awareness improves reliability of voice commands"}),"\n",(0,o.jsx)(n.li,{children:"Proper error handling is essential for usable systems"}),"\n"]})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}}}]);