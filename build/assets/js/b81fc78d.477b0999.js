"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[478],{963:(e,n,r)=>{r.d(n,{A:()=>s});r(6540);var a=r(4164);const i={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var t=r(4848);function s({type:e,children:n}){const r=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),s=(0,a.A)("callout",i.callout,i[e]);return(0,t.jsxs)("div",{className:s,children:[(0,t.jsx)("div",{className:i.icon,children:r}),(0,t.jsx)("div",{className:i.content,children:n})]})}},3816:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-4-vla/chapter-19-multi-modal-interaction","title":"Chapter 19 - Multi-Modal Interaction","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-19-multi-modal-interaction.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-19-multi-modal-interaction","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-19-multi-modal-interaction","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 19 - Multi-Modal Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 18 - Cognitive Planning","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-18-cognitive-planning"},"next":{"title":"Chapter 20 - Capstone Project","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-20-capstone-project"}}');var i=r(4848),t=r(8453),s=r(963);const o={sidebar_position:4,title:"Chapter 19 - Multi-Modal Interaction"},l="Chapter 19: Multi-Modal Interaction in Robotics",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-19-multi-modal-interaction-in-robotics",children:"Chapter 19: Multi-Modal Interaction in Robotics"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design multi-modal interfaces that combine different sensing and communication modalities"}),"\n",(0,i.jsx)(n.li,{children:"Integrate visual, auditory, and haptic feedback for enhanced interaction"}),"\n",(0,i.jsx)(n.li,{children:"Implement cross-modal reasoning for robust human-robot interaction"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction in robotics combines multiple sensory channels (vision, audio, touch, etc.) to create more natural and robust human-robot interfaces."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CompressedImage, LaserScan\r\nfrom std_msgs.msg import String, Float32MultiArray\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\nimport json\r\n\r\nclass MultiModalInteractionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('multi_modal_interaction')\r\n        \r\n        # Initialize CvBridge for image processing\r\n        self.bridge = CvBridge()\r\n        \r\n        # Internal state for multi-modal fusion\r\n        self.vision_data = None\r\n        self.audio_input = \"\"\r\n        self.tactile_data = None\r\n        self.fusion_result = None\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.interaction_pub = self.create_publisher(String, '/interaction_result', 10)\r\n        self.fusion_pub = self.create_publisher(Float32MultiArray, '/fused_data', 10)\r\n        \r\n        # Subscribers for different modalities\r\n        self.image_sub = self.create_subscription(\r\n            Image, \r\n            '/camera/rgb/image_raw', \r\n            self.image_callback, \r\n            10\r\n        )\r\n        \r\n        self.compressed_image_sub = self.create_subscription(\r\n            CompressedImage, \r\n            '/camera/rgb/image_compressed', \r\n            self.compressed_image_callback, \r\n            10\r\n        )\r\n        \r\n        self.audio_sub = self.create_subscription(\r\n            String, \r\n            '/audio_transcription', \r\n            self.audio_callback, \r\n            10\r\n        )\r\n        \r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n        \r\n        # Timer for multi-modal fusion\r\n        self.fusion_timer = self.create_timer(0.5, self.multi_modal_fusion)\r\n\r\n    def image_callback(self, msg: Image):\r\n        \"\"\"Process RGB image data\"\"\"\r\n        try:\r\n            # Convert ROS Image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n            \r\n            # Perform basic object detection\r\n            objects = self.detect_objects(cv_image)\r\n            \r\n            # Store vision data\r\n            self.vision_data = {\r\n                'timestamp': self.get_clock().now().nanoseconds,\r\n                'objects': objects,\r\n                'image_shape': cv_image.shape\r\n            }\r\n            \r\n            self.get_logger().info(f'Detected {len(objects)} objects in image')\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n\r\n    def compressed_image_callback(self, msg: CompressedImage):\r\n        \"\"\"Process compressed image data (alternative to raw image)\"\"\"\r\n        try:\r\n            # Convert compressed image to OpenCV\r\n            np_arr = np.frombuffer(msg.data, np.uint8)\r\n            cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\r\n            \r\n            # Extract visual features\r\n            features = self.extract_visual_features(cv_image)\r\n            \r\n            # Update vision data with features\r\n            if self.vision_data:\r\n                self.vision_data['features'] = features\r\n            else:\r\n                self.vision_data = {\r\n                    'timestamp': self.get_clock().now().nanoseconds,\r\n                    'features': features,\r\n                    'image_shape': cv_image.shape\r\n                }\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing compressed image: {e}')\r\n\r\n    def audio_callback(self, msg: String):\r\n        \"\"\"Process audio transcription\"\"\"\r\n        self.audio_input = msg.data\r\n        self.get_logger().info(f'Audio input: {self.audio_input}')\r\n\r\n    def lidar_callback(self, msg: LaserScan):\r\n        \"\"\"Process LIDAR data for spatial awareness\"\"\"\r\n        # Extract relevant information from LIDAR scan\r\n        ranges = np.array(msg.ranges)\r\n        \r\n        # Filter out invalid ranges\r\n        valid_ranges = ranges[(ranges > msg.range_min) & (ranges < msg.range_max)]\r\n        \r\n        # Calculate basic spatial features\r\n        if len(valid_ranges) > 0:\r\n            nearest_obstacle = np.min(valid_ranges)\r\n            free_space_ahead = np.mean(valid_ranges[len(valid_ranges)//2 - 10:len(valid_ranges)//2 + 10])\r\n        else:\r\n            nearest_obstacle = float('inf')\r\n            free_space_ahead = float('inf')\r\n        \r\n        # Store LIDAR data\r\n        self.lidar_data = {\r\n            'timestamp': self.get_clock().now().nanoseconds,\r\n            'nearest_obstacle': nearest_obstacle,\r\n            'free_space_ahead': free_space_ahead,\r\n            'valid_ranges_count': len(valid_ranges)\r\n        }\r\n\r\n    def detect_objects(self, image):\r\n        \"\"\"Detect objects in image using simple color-based segmentation\"\"\"\r\n        # Convert to HSV for color-based segmentation\r\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\r\n        \r\n        # Define color ranges for common objects\r\n        color_ranges = {\r\n            'red': ([0, 50, 50], [10, 255, 255]),\r\n            'blue': ([100, 50, 50], [130, 255, 255]),\r\n            'green': ([40, 50, 50], [80, 255, 255])\r\n        }\r\n        \r\n        objects = []\r\n        for color_name, (lower, upper) in color_ranges.items():\r\n            # Create mask for color range\r\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\r\n            \r\n            # Find contours\r\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n            \r\n            # Filter contours by area\r\n            for cnt in contours:\r\n                area = cv2.contourArea(cnt)\r\n                if area > 500:  # Only consider objects larger than 500 pixels\r\n                    # Get bounding box\r\n                    x, y, w, h = cv2.boundingRect(cnt)\r\n                    objects.append({\r\n                        'color': color_name,\r\n                        'bbox': [x, y, x+w, y+h],\r\n                        'area': area\r\n                    })\r\n        \r\n        return objects\r\n\r\n    def extract_visual_features(self, image):\r\n        \"\"\"Extract visual features from image\"\"\"\r\n        # Convert to grayscale\r\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n        \r\n        # Compute histogram\r\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\r\n        \r\n        # Compute edges\r\n        edges = cv2.Canny(gray, 50, 150)\r\n        \r\n        # Calculate basic statistics\r\n        features = {\r\n            'mean_intensity': float(np.mean(gray)),\r\n            'std_intensity': float(np.std(gray)),\r\n            'edge_density': float(np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])),\r\n            'histogram': hist.flatten().tolist()\r\n        }\r\n        \r\n        return features\r\n\r\n    def multi_modal_fusion(self):\r\n        \"\"\"Fuse data from multiple modalities\"\"\"\r\n        # Check if we have data from all modalities\r\n        all_modalities_available = True\r\n        modalities = []\r\n        \r\n        if self.vision_data:\r\n            modalities.append(('vision', self.vision_data))\r\n        else:\r\n            all_modalities_available = False\r\n            \r\n        if self.audio_input:\r\n            modalities.append(('audio', self.audio_input))\r\n        else:\r\n            all_modalities_available = False\r\n            \r\n        if hasattr(self, 'lidar_data') and self.lidar_data:\r\n            modalities.append(('lidar', self.lidar_data))\r\n        else:\r\n            all_modalities_available = False\r\n        \r\n        # Perform fusion if all modalities are available\r\n        if all_modalities_available:\r\n            self.get_logger().info('Performing multi-modal fusion')\r\n            \r\n            # Simple fusion: combine all modalities into a single representation\r\n            fusion_result = {\r\n                'timestamp': self.get_clock().now().nanoseconds,\r\n                'fused_modalities': [m[0] for m in modalities],\r\n                'spatial_context': self.lidar_data['free_space_ahead'],\r\n                'visual_content': [obj['color'] for obj in self.vision_data['objects']],\r\n                'audio_content': self.audio_input\r\n            }\r\n            \r\n            # Cross-modal reasoning: if audio mentions an object color, find it in vision\r\n            if self.audio_input:\r\n                for obj in self.vision_data['objects']:\r\n                    if obj['color'] in self.audio_input.lower():\r\n                        fusion_result['target_object'] = obj\r\n                        self.get_logger().info(f'Found target object based on audio: {obj}')\r\n                        \r\n            self.fusion_result = fusion_result\r\n            \r\n            # Publish fused data\r\n            fusion_msg = Float32MultiArray()\r\n            # Convert fusion result to array format for publishing\r\n            # (in a real implementation, this would be more sophisticated)\r\n            fusion_msg.data = [float(self.lidar_data['free_space_ahead']), \r\n                              len(self.vision_data['objects']), \r\n                              len(self.audio_input)]\r\n            self.fusion_pub.publish(fusion_msg)\r\n            \r\n            # Publish interaction result\r\n            result_msg = String()\r\n            result_msg.data = json.dumps(fusion_result)\r\n            self.interaction_pub.publish(result_msg)\r\n        else:\r\n            # Not all modalities available, but still try to make sense of available data\r\n            partial_fusion = {\r\n                'timestamp': self.get_clock().now().nanoseconds,\r\n                'available_modalities': [m[0] for m in modalities],\r\n                'is_complete': False\r\n            }\r\n            \r\n            self.fusion_result = partial_fusion\r\n            \r\n            result_msg = String()\r\n            result_msg.data = json.dumps(partial_fusion)\r\n            self.interaction_pub.publish(result_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    interaction_node = MultiModalInteractionNode()\r\n    \r\n    try:\r\n        rclpy.spin(interaction_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        interaction_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\r\n    A[Human User] --\x3e B[Visual Modality]\r\n    A --\x3e C[Audio Modality]\r\n    A --\x3e D[Tactile Modality]\r\n    A --\x3e E[Spatial Modality]\r\n    B --\x3e F[Multi-Modal Fusion]\r\n    C --\x3e F\r\n    D --\x3e F\r\n    E --\x3e F\r\n    F --\x3e G[Cross-Modal Reasoning]\r\n    G --\x3e H[Unified Representation]\r\n    H --\x3e I[Robot Action]\r\n    I --\x3e J[Robot Feedback]\r\n    J --\x3e A\n"})}),"\n",(0,i.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,i.jsx)(s.A,{type:"info",children:(0,i.jsx)(n.p,{children:"Multi-modal systems can provide robustness by using alternative modalities when one fails or is ambiguous."})}),"\n",(0,i.jsx)(s.A,{type:"tip",children:(0,i.jsx)(n.p,{children:"Implement cross-modal attention mechanisms that allow information from one modality to guide processing in another."})}),"\n",(0,i.jsx)(s.A,{type:"caution",children:(0,i.jsx)(n.p,{children:"Fusing information across modalities requires careful temporal alignment and handling of different data rates."})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a multi-modal system that combines vision and speech for object manipulation"}),"\n",(0,i.jsx)(n.li,{children:"Create a cross-modal attention mechanism for a robot interface"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate the robustness of multi-modal vs. single-modal interaction"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-modal interaction combines different sensory channels for richer interaction"}),"\n",(0,i.jsx)(n.li,{children:"Cross-modal reasoning enables more robust and natural interaction"}),"\n",(0,i.jsx)(n.li,{children:"Temporal alignment is critical when fusing different modalities"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var a=r(6540);const i={},t=a.createContext(i);function s(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);