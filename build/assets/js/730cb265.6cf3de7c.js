"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[669],{963:(e,n,r)=>{r.d(n,{A:()=>s});r(6540);var t=r(4164);const a={callout:"callout_jYHE",info:"info_IOty",tip:"tip_s2nh",caution:"caution_w7Js",danger:"danger_zfsw",icon:"icon_Ghiv",content:"content_JMk4"};var o=r(4848);function s({type:e,children:n}){const r=function(e){switch(e){case"info":default:return"\u2139\ufe0f";case"tip":return"\ud83d\udca1";case"caution":return"\u26a0\ufe0f";case"danger":return"\u274c"}}(e),s=(0,t.A)("callout",a.callout,a[e]);return(0,o.jsxs)("div",{className:s,children:[(0,o.jsx)("div",{className:a.icon,children:r}),(0,o.jsx)("div",{className:a.content,children:n})]})}},2761:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-4-vla/chapter-16-llms-and-robotics","title":"Chapter 16 - LLMs and Robotics","description":"Learning Objectives","source":"@site/docs/module-4-vla/chapter-16-llms-and-robotics.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-16-llms-and-robotics","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-16-llms-and-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedBy":"Muhammad Uzair","lastUpdatedAt":1765045729000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 16 - LLMs and Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 15 - Nav2","permalink":"/humanoid-robotics/docs/module-3-isaac/chapter-15-nav2"},"next":{"title":"Chapter 17 - Voice to Action","permalink":"/humanoid-robotics/docs/module-4-vla/chapter-17-voice-to-action"}}');var a=r(4848),o=r(8453),s=r(963);const i={sidebar_position:1,title:"Chapter 16 - LLMs and Robotics"},l="Chapter 16: Large Language Models and Robotics",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content with Code Examples",id:"content-with-code-examples",level:2},{value:"Mermaid Diagrams",id:"mermaid-diagrams",level:2},{value:"Callouts",id:"callouts",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-16-large-language-models-and-robotics",children:"Chapter 16: Large Language Models and Robotics"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand how large language models (LLMs) can be integrated with robotics systems"}),"\n",(0,a.jsx)(n.li,{children:"Implement natural language interfaces for robot control"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the benefits and challenges of LLM-robot integration"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"content-with-code-examples",children:"Content with Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models (LLMs) like GPT, Claude, and specialized models are increasingly being integrated with robotics systems to enable natural language interaction, task planning, and high-level reasoning."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom std_msgs.msg import String\r\nimport json\r\n\r\nclass LLMRobotController(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_robot_controller\')\r\n        \r\n        # Set OpenAI API key (in a real application, this would be in a secure config)\r\n        openai.api_key = "YOUR_API_KEY"\r\n        \r\n        # Create publishers and subscribers\r\n        self.nav_goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\r\n        self.cmd_pub = self.create_publisher(String, \'/robot_command\', 10)\r\n        self.voice_sub = self.create_subscription(\r\n            String, \r\n            \'/voice_command\', \r\n            self.voice_command_callback, \r\n            10\r\n        )\r\n        \r\n        # Timer to process high-level commands\r\n        self.timer = self.create_timer(1.0, self.process_commands)\r\n\r\n        self.pending_commands = []\r\n\r\n    def voice_command_callback(self, msg: String):\r\n        """Handle incoming voice commands"""\r\n        command_text = msg.data\r\n        self.get_logger().info(f\'Received voice command: {command_text}\')\r\n        \r\n        # Process command through LLM to generate robot actions\r\n        robot_action = self.process_command_with_llm(command_text)\r\n        self.pending_commands.append(robot_action)\r\n\r\n    def process_command_with_llm(self, command: str) -> dict:\r\n        """Process natural language command using LLM"""\r\n        prompt = f"""\r\n        Given the following robot command, convert it into structured robot actions.\r\n        The robot capabilities are: navigation, object manipulation, grasping, speaking, and light control.\r\n        Respond in JSON format with action_type and parameters.\r\n        \r\n        Command: "{command}"\r\n        \r\n        Example response structure:\r\n        {{\r\n            "action_type": "navigation",\r\n            "parameters": {{\r\n                "destination": [x, y, theta]\r\n            }}\r\n        }}\r\n        \r\n        Response:\r\n        """\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[{"role": "user", "content": prompt}],\r\n                temperature=0.1\r\n            )\r\n            \r\n            # Parse the LLM response\r\n            action_str = response.choices[0].message[\'content\'].strip()\r\n            \r\n            # Extract JSON from response (in a real implementation, this would be more robust)\r\n            start_idx = action_str.find(\'{\')\r\n            end_idx = action_str.rfind(\'}\') + 1\r\n            json_str = action_str[start_idx:end_idx]\r\n            \r\n            return json.loads(json_str)\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing command with LLM: {e}\')\r\n            return {"action_type": "error", "parameters": {}}\r\n\r\n    def process_commands(self):\r\n        """Process pending commands"""\r\n        if not self.pending_commands:\r\n            return\r\n            \r\n        command = self.pending_commands.pop(0)\r\n        \r\n        if command["action_type"] == "navigation":\r\n            self.execute_navigation(command["parameters"])\r\n        elif command["action_type"] == "manipulation":\r\n            self.execute_manipulation(command["parameters"])\r\n        elif command["action_type"] == "speak":\r\n            self.execute_speak(command["parameters"])\r\n\r\n    def execute_navigation(self, params):\r\n        """Execute navigation command"""\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n        \r\n        dest = params.get("destination", [0, 0, 0])\r\n        pose_msg.pose.position.x = float(dest[0])\r\n        pose_msg.pose.position.y = float(dest[1])\r\n        pose_msg.pose.position.z = 0.0\r\n        \r\n        # Convert theta to quaternion\r\n        theta = dest[2]\r\n        pose_msg.pose.orientation.z = math.sin(theta / 2.0)\r\n        pose_msg.pose.orientation.w = math.cos(theta / 2.0)\r\n        \r\n        self.nav_goal_pub.publish(pose_msg)\r\n        self.get_logger().info(f\'Navigating to: {dest}\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    controller = LLMRobotController()\r\n    \r\n    try:\r\n        rclpy.spin(controller)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        controller.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"mermaid-diagrams",children:"Mermaid Diagrams"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD;\r\n    A[User Natural Language] --\x3e B[Large Language Model]\r\n    B --\x3e C[Structured Robot Actions]\r\n    C --\x3e D[Navigation]\r\n    C --\x3e E[Manipulation]\r\n    C --\x3e F[Communication]\r\n    D --\x3e G[Robot Movement]\r\n    E --\x3e H[Robot Manipulator]\r\n    F --\x3e I[Speech/Display]\r\n    J[Robot Sensors] --\x3e B\r\n    K[Environment State] --\x3e B\n"})}),"\n",(0,a.jsx)(n.h2,{id:"callouts",children:"Callouts"}),"\n",(0,a.jsx)(s.A,{type:"info",children:(0,a.jsx)(n.p,{children:"LLMs can bridge the gap between high-level human instructions and low-level robot actions, enabling more intuitive human-robot interaction."})}),"\n",(0,a.jsx)(s.A,{type:"tip",children:(0,a.jsx)(n.p,{children:"When using LLMs with robots, implement safety checks to ensure that generated actions are appropriate and safe for the environment."})}),"\n",(0,a.jsx)(s.A,{type:"caution",children:(0,a.jsx)(n.p,{children:"LLMs can generate plausible but incorrect responses. Always validate and verify commands before execution on physical robots."})}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement a natural language interface for a simulated robot"}),"\n",(0,a.jsx)(n.li,{children:"Create a safety validation system for LLM-generated commands"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the effectiveness of LLM-based task planning vs. traditional methods"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LLMs enable natural language interaction with robots"}),"\n",(0,a.jsx)(n.li,{children:"They can generate complex action sequences from high-level commands"}),"\n",(0,a.jsx)(n.li,{children:"Safety validation is critical when executing LLM-generated commands"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>i});var t=r(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);